{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multiclass      import OneVsOneClassifier\n",
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "development_trend_score\n",
      "3    4019\n",
      "2    2018\n",
      "4    1949\n",
      "1    1018\n",
      "5     996\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('./data/urban_development_dataset.csv')\n",
    "print(train_df['development_trend_score'].value_counts())\n",
    "test_df = pd.read_csv('./data/urban_development_test_data.csv')\n",
    "common_columns = train_df.columns.intersection(test_df.columns)\n",
    "#definition of the X and Y\n",
    "Y = train_df['development_trend_score']\n",
    "# Y -= 1\n",
    "X = train_df[common_columns]\n",
    "\n",
    "test_data = pd.read_csv('./data/urban_development_test_data.csv')\n",
    "test_data = test_data[common_columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null values traitment / Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "development_trend_score\n",
      "3    2831\n",
      "2    2077\n",
      "4    1890\n",
      "1     999\n",
      "5     997\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_54650/1289462203.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[numerical_features] = knn_imputer.fit_transform(X[numerical_features])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "numerical_features = [\n",
    "    \"population_density\", 'median_age', 'average_household_income', \n",
    "    'number_of_schools', 'number_of_hospitals', 'number_of_parks', \n",
    "    'public_transport_availability', 'unemployment_rate', 'retail_density', \n",
    "    'office_space_availability', 'green_space_percentage', 'average_temperature', \n",
    "    'air_quality_index', 'crime_rate_per_1000', 'fire_station_proximity', \n",
    "    'proximity_to_highways', 'internet_speed', 'road_density', \n",
    "    'housing_price_index', 'year'\n",
    "]\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "X[numerical_features] = knn_imputer.fit_transform(X[numerical_features])\n",
    "# print(X.isnull().sum())\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "\n",
    "class_3 = X_scaled[Y == 3]\n",
    "other_classes = X_scaled[Y != 3]\n",
    "Y_class_3 = Y[Y == 3]\n",
    "Y_other_classes = Y[Y != 3]\n",
    "\n",
    "# Determine the desired count for class 3 (e.g., reduce it by 30%)\n",
    "desired_count = int(len(class_3) * 0.7)\n",
    "\n",
    "# Randomly undersample class 3\n",
    "class_3_downsampled = resample(\n",
    "    class_3,\n",
    "    replace=False,  # Without replacement\n",
    "    n_samples=desired_count,\n",
    "    random_state=42  # Ensure reproducibility\n",
    ")\n",
    "Y_class_3_downsampled = resample(\n",
    "    Y_class_3,\n",
    "    replace=False,\n",
    "    n_samples=desired_count,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Combine the downsampled class 3 with other classes\n",
    "X_balanced = pd.concat([class_3_downsampled, other_classes])\n",
    "Y_balanced = pd.concat([Y_class_3_downsampled, Y_other_classes])\n",
    "\n",
    "# Shuffle the data to mix the rows\n",
    "X_balanced, Y_balanced = resample(\n",
    "    X_balanced,\n",
    "    Y_balanced,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_scaled = X_balanced\n",
    "\n",
    "# Print the new value counts\n",
    "print(Y_balanced.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Initialize PCA and transform data\n",
    "# pca = PCA()\n",
    "# X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# # Explained variance and cumulative variance\n",
    "# explained_variance = pca.explained_variance_ratio_\n",
    "# cumulative_variance = explained_variance.cumsum()\n",
    "\n",
    "# # Plot the cumulative explained variance\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(range(1, len(explained_variance) + 1), cumulative_variance, marker='o', linestyle='--')\n",
    "# plt.title('Explained Variance by Principal Components')\n",
    "# plt.xlabel('Number of Components')\n",
    "# plt.ylabel('Cumulative Explained Variance')\n",
    "# plt.grid()\n",
    "# plt.show()\n",
    "\n",
    "# # Set number of components to 5 (most relevant features)\n",
    "# n_components = 5\n",
    "# pca = PCA(n_components=n_components)\n",
    "# X_reduced = pca.fit_transform(X_scaled)\n",
    "X_reduced = X_scaled\n",
    "\n",
    "# Print resulting dataset shape and verify the transformation\n",
    "print(f\"Shape of reduced dataset: {X_reduced.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11268, 20) (11268,)\n",
      "(2817, 20) (2817,)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "smote_tomek = SMOTETomek(random_state=42)\n",
    "NewX, NewY = smote_tomek.fit_resample(X_scaled, Y_balanced)\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(NewX, NewY, test_size=0.2, random_state=42)\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.21780445111277819\n",
      "Classification Report for SVM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.22      0.32      0.26       768\n",
      "           2       0.21      0.15      0.17       819\n",
      "           3       0.23      0.19      0.21       806\n",
      "           4       0.19      0.14      0.16       807\n",
      "           5       0.23      0.30      0.26       799\n",
      "\n",
      "    accuracy                           0.22      3999\n",
      "   macro avg       0.22      0.22      0.21      3999\n",
      "weighted avg       0.22      0.22      0.21      3999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "\n",
    "\n",
    "model = svm.SVC(kernel='linear', gamma='scale' , C=0.1)\n",
    "model.fit(X_train,Y_train)\n",
    "\n",
    "# ovo_model = OneVsOneClassifier(model)\n",
    "# ovo_model.fit(train_x, train_y)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f\"Test Accuracy: {accuracy_score(Y_test, y_pred)}\")\n",
    "print(f\"Classification Report for SVM:\\n{classification_report(Y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation crois√©e (5 folds) :\n",
      "test_accuracy: 0.8392 (+/- 0.0420)\n",
      "test_balanced_accuracy: 0.8389 (+/- 0.0419)\n",
      "test_f1_macro: 0.8396 (+/- 0.0391)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'balanced_accuracy': 'balanced_accuracy',\n",
    "    'f1_macro': make_scorer(f1_score, average='macro')\n",
    "}\n",
    "\n",
    "rfm = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=None,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "cv_results = cross_validate(\n",
    "    rfm,\n",
    "    NewX, NewY,\n",
    "    cv=5, \n",
    "    scoring=scoring,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"Validation crois√©e (5 folds) :\")\n",
    "for metric, scores in cv_results.items():\n",
    "    if \"test\" in metric:  \n",
    "        print(f\"{metric}: {scores.mean():.4f} (+/- {scores.std():.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best parameters (grid search for RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for Random Forest: {'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "Best Cross-Validation Score for Random Forest: 0.5874596748984057\n",
      "Test Accuracy for Random Forest: 0.5896474118529632\n",
      "Classification Report for Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.68      0.74      0.71       768\n",
      "           2       0.56      0.47      0.51       819\n",
      "           3       0.45      0.50      0.47       806\n",
      "           4       0.58      0.54      0.56       807\n",
      "           5       0.68      0.72      0.70       799\n",
      "\n",
      "    accuracy                           0.59      3999\n",
      "   macro avg       0.59      0.59      0.59      3999\n",
      "weighted avg       0.59      0.59      0.59      3999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200],   \n",
    "    'max_depth': [None, 10, 20],      \n",
    "    'min_samples_split': [5, 10],  \n",
    "    'min_samples_leaf': [2, 4],    \n",
    "}\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "grid_search_rf = GridSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_grid=param_grid_rf,\n",
    "    cv=5, \n",
    "    scoring='f1_micro', \n",
    "    verbose=1,\n",
    "    n_jobs=-1  \n",
    ")\n",
    "\n",
    "grid_search_rf.fit(X_train, Y_train)\n",
    "\n",
    "print(f\"Best Parameters for Random Forest: {grid_search_rf.best_params_}\")\n",
    "print(f\"Best Cross-Validation Score for Random Forest: {grid_search_rf.best_score_}\")\n",
    "\n",
    "best_rf_model = grid_search_rf.best_estimator_\n",
    "y_pred_rf = best_rf_model.predict(X_test)\n",
    "\n",
    "print(f\"Test Accuracy for Random Forest: {accuracy_score(Y_test, y_pred_rf)}\")\n",
    "print(f\"Classification Report for Random Forest:\\n{classification_report(Y_test, y_pred_rf)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "train_y = to_categorical(Y_train)\n",
    "test_y = to_categorical(Y_test)\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),  \n",
    "    Dropout(0.3),  \n",
    "    Dense(64, activation='relu'), \n",
    "    Dropout(0.3),\n",
    "    Dense(train_y.shape[1], activation='softmax')  \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X_train, train_y, validation_split=0.2, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# √âvaluation du mod√®le\n",
    "test_loss, test_accuracy = model.evaluate(X_test, test_y)\n",
    "print(f\"Loss sur le test : {test_loss:.4f}\")\n",
    "print(f\"Pr√©cision sur le test : {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved at: ./test/development_trend_predictions_classification.csv\n"
     ]
    }
   ],
   "source": [
    "output = rfm.predict(test_data) #here changing the model \n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": test_data.index + 1,  \n",
    "    \"development_trend_score\": output\n",
    "})\n",
    "submission_file_path = './test/development_trend_predictions_classification.csv'\n",
    "submission.to_csv(submission_file_path, index=False)\n",
    "print(f\"Submission file saved at: {submission_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
